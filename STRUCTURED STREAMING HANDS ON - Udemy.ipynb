{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f44e94-d692-4c51-ab2f-e157ec35f74c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files = dbutils.fs.ls('mnt/demo-datasets/bookstore') # finding all the files in the directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bebdadc-ee8e-4a38-8773-17e9583010af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/demo-datasets/bookstore/books-cdc/</td><td>books-cdc/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/demo-datasets/bookstore/books-csv/</td><td>books-csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/demo-datasets/bookstore/books-csv-new/</td><td>books-csv-new/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/demo-datasets/bookstore/books-streaming/</td><td>books-streaming/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/demo-datasets/bookstore/customers-json/</td><td>customers-json/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/demo-datasets/bookstore/customers-json-new/</td><td>customers-json-new/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/demo-datasets/bookstore/orders/</td><td>orders/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/demo-datasets/bookstore/orders-json-raw/</td><td>orders-json-raw/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/demo-datasets/bookstore/orders-json-streaming/</td><td>orders-json-streaming/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/demo-datasets/bookstore/orders-new/</td><td>orders-new/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/demo-datasets/bookstore/orders-raw/</td><td>orders-raw/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/demo-datasets/bookstore/orders-streaming/</td><td>orders-streaming/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/demo-datasets/bookstore/books-cdc/",
         "books-cdc/",
         0,
         0
        ],
        [
         "dbfs:/mnt/demo-datasets/bookstore/books-csv/",
         "books-csv/",
         0,
         0
        ],
        [
         "dbfs:/mnt/demo-datasets/bookstore/books-csv-new/",
         "books-csv-new/",
         0,
         0
        ],
        [
         "dbfs:/mnt/demo-datasets/bookstore/books-streaming/",
         "books-streaming/",
         0,
         0
        ],
        [
         "dbfs:/mnt/demo-datasets/bookstore/customers-json/",
         "customers-json/",
         0,
         0
        ],
        [
         "dbfs:/mnt/demo-datasets/bookstore/customers-json-new/",
         "customers-json-new/",
         0,
         0
        ],
        [
         "dbfs:/mnt/demo-datasets/bookstore/orders/",
         "orders/",
         0,
         0
        ],
        [
         "dbfs:/mnt/demo-datasets/bookstore/orders-json-raw/",
         "orders-json-raw/",
         0,
         0
        ],
        [
         "dbfs:/mnt/demo-datasets/bookstore/orders-json-streaming/",
         "orders-json-streaming/",
         0,
         0
        ],
        [
         "dbfs:/mnt/demo-datasets/bookstore/orders-new/",
         "orders-new/",
         0,
         0
        ],
        [
         "dbfs:/mnt/demo-datasets/bookstore/orders-raw/",
         "orders-raw/",
         0,
         0
        ],
        [
         "dbfs:/mnt/demo-datasets/bookstore/orders-streaming/",
         "orders-streaming/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7640d52f-1cb6-432c-a300-6c540d08f132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE TABLE books\n",
    "  (book_id STRING, title STRING, author STRING, category STRING, price DOUBLE)\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "  header = \"true\",\n",
    "  delimiter = \";\",\n",
    "  path = 'dbfs:/mnt/demo-datasets/bookstore/books-csv/export_*.csv'\n",
    ");\n",
    "-- LOCATION 'dbfs:/mnt/demo-datasets/bookstore/books/'\n",
    "\n",
    "--dbfs:/mnt/demo-datasets/bookstore/books-csv/export_001.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72b98358-62ef-4aea-9e13-be9e15a94da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>book_id</th><th>title</th><th>author</th><th>category</th><th>price</th></tr></thead><tbody><tr><td>B07</td><td>The Hundred-Page Machine Learning</td><td>Andriy Burkov</td><td>Computer Science</td><td>33.0</td></tr><tr><td>B08</td><td>Quantum Computing for Everyone</td><td>Chris Bernhardt</td><td>Computer Science</td><td>41.0</td></tr><tr><td>B09</td><td>Advanced Data Structures</td><td>Peter Brass</td><td>Computer Science</td><td>24.0</td></tr><tr><td>B10</td><td>Beginning Database Design Solutions</td><td>Rod Stephens</td><td>Computer Science</td><td>44.0</td></tr><tr><td>B11</td><td>Business Intelligence for Dummies</td><td>Swain Scheps</td><td>Computer Science</td><td>38.0</td></tr><tr><td>B12</td><td>Big Data in Practice</td><td>Bernard Marr</td><td>Computer Science</td><td>30.0</td></tr><tr><td>B01</td><td>The Soul of a New Machine</td><td>Tracy Kidder</td><td>Computer Science</td><td>49.0</td></tr><tr><td>B02</td><td>Learning JavaScript Design Patterns</td><td>Addy Osmani</td><td>Computer Science</td><td>28.0</td></tr><tr><td>B03</td><td>Make Your Own Neural Network</td><td>Tariq Rashid</td><td>Computer Science</td><td>35.0</td></tr><tr><td>B04</td><td>Robot Dynamics and Control</td><td>Mark W. Spong</td><td>Computer Science</td><td>20.0</td></tr><tr><td>B05</td><td>Fluent Python</td><td>Luciano Ramalho</td><td>Computer Science</td><td>47.0</td></tr><tr><td>B06</td><td>Deep Learning with Python</td><td>François Chollet</td><td>Computer Science</td><td>22.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "B07",
         "The Hundred-Page Machine Learning",
         "Andriy Burkov",
         "Computer Science",
         33.0
        ],
        [
         "B08",
         "Quantum Computing for Everyone",
         "Chris Bernhardt",
         "Computer Science",
         41.0
        ],
        [
         "B09",
         "Advanced Data Structures",
         "Peter Brass",
         "Computer Science",
         24.0
        ],
        [
         "B10",
         "Beginning Database Design Solutions",
         "Rod Stephens",
         "Computer Science",
         44.0
        ],
        [
         "B11",
         "Business Intelligence for Dummies",
         "Swain Scheps",
         "Computer Science",
         38.0
        ],
        [
         "B12",
         "Big Data in Practice",
         "Bernard Marr",
         "Computer Science",
         30.0
        ],
        [
         "B01",
         "The Soul of a New Machine",
         "Tracy Kidder",
         "Computer Science",
         49.0
        ],
        [
         "B02",
         "Learning JavaScript Design Patterns",
         "Addy Osmani",
         "Computer Science",
         28.0
        ],
        [
         "B03",
         "Make Your Own Neural Network",
         "Tariq Rashid",
         "Computer Science",
         35.0
        ],
        [
         "B04",
         "Robot Dynamics and Control",
         "Mark W. Spong",
         "Computer Science",
         20.0
        ],
        [
         "B05",
         "Fluent Python",
         "Luciano Ramalho",
         "Computer Science",
         47.0
        ],
        [
         "B06",
         "Deep Learning with Python",
         "François Chollet",
         "Computer Science",
         22.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "book_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "author",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM books;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f3abc99-c081-49ba-b5f1-4a7292083f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "READING DELTA TABLE AS STREAM OF DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c7dd732-ca51-44ee-a4df-d01c5027e24a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream\n",
    " .table(\"books\")\n",
    " .createOrReplaceTempView(\"books_streaming_temp_vw\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "306fbb27-b874-4178-bc21-92033d99bbc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Querying the temp view "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a755c78-dd46-4eca-b433-61bba4c70e92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>book_id</th><th>title</th><th>author</th><th>category</th><th>price</th></tr></thead><tbody><tr><td>B07</td><td>The Hundred-Page Machine Learning</td><td>Andriy Burkov</td><td>Computer Science</td><td>33.0</td></tr><tr><td>B08</td><td>Quantum Computing for Everyone</td><td>Chris Bernhardt</td><td>Computer Science</td><td>41.0</td></tr><tr><td>B09</td><td>Advanced Data Structures</td><td>Peter Brass</td><td>Computer Science</td><td>24.0</td></tr><tr><td>B10</td><td>Beginning Database Design Solutions</td><td>Rod Stephens</td><td>Computer Science</td><td>44.0</td></tr><tr><td>B11</td><td>Business Intelligence for Dummies</td><td>Swain Scheps</td><td>Computer Science</td><td>38.0</td></tr><tr><td>B12</td><td>Big Data in Practice</td><td>Bernard Marr</td><td>Computer Science</td><td>30.0</td></tr><tr><td>B01</td><td>The Soul of a New Machine</td><td>Tracy Kidder</td><td>Computer Science</td><td>49.0</td></tr><tr><td>B02</td><td>Learning JavaScript Design Patterns</td><td>Addy Osmani</td><td>Computer Science</td><td>28.0</td></tr><tr><td>B03</td><td>Make Your Own Neural Network</td><td>Tariq Rashid</td><td>Computer Science</td><td>35.0</td></tr><tr><td>B04</td><td>Robot Dynamics and Control</td><td>Mark W. Spong</td><td>Computer Science</td><td>20.0</td></tr><tr><td>B05</td><td>Fluent Python</td><td>Luciano Ramalho</td><td>Computer Science</td><td>47.0</td></tr><tr><td>B06</td><td>Deep Learning with Python</td><td>François Chollet</td><td>Computer Science</td><td>22.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "B07",
         "The Hundred-Page Machine Learning",
         "Andriy Burkov",
         "Computer Science",
         33.0
        ],
        [
         "B08",
         "Quantum Computing for Everyone",
         "Chris Bernhardt",
         "Computer Science",
         41.0
        ],
        [
         "B09",
         "Advanced Data Structures",
         "Peter Brass",
         "Computer Science",
         24.0
        ],
        [
         "B10",
         "Beginning Database Design Solutions",
         "Rod Stephens",
         "Computer Science",
         44.0
        ],
        [
         "B11",
         "Business Intelligence for Dummies",
         "Swain Scheps",
         "Computer Science",
         38.0
        ],
        [
         "B12",
         "Big Data in Practice",
         "Bernard Marr",
         "Computer Science",
         30.0
        ],
        [
         "B01",
         "The Soul of a New Machine",
         "Tracy Kidder",
         "Computer Science",
         49.0
        ],
        [
         "B02",
         "Learning JavaScript Design Patterns",
         "Addy Osmani",
         "Computer Science",
         28.0
        ],
        [
         "B03",
         "Make Your Own Neural Network",
         "Tariq Rashid",
         "Computer Science",
         35.0
        ],
        [
         "B04",
         "Robot Dynamics and Control",
         "Mark W. Spong",
         "Computer Science",
         20.0
        ],
        [
         "B05",
         "Fluent Python",
         "Luciano Ramalho",
         "Computer Science",
         47.0
        ],
        [
         "B06",
         "Deep Learning with Python",
         "François Chollet",
         "Computer Science",
         22.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "book_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "author",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM books_streaming_temp_vw;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79bb0cea-4386-4d24-92e3-e2e1be98d614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Some Aggregation on streaming temp view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3049369-25d6-4166-a640-32c6b34638d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>author</th><th>total_books</th></tr></thead><tbody><tr><td>Mark W. Spong</td><td>1</td></tr><tr><td>Chris Bernhardt</td><td>1</td></tr><tr><td>Tariq Rashid</td><td>1</td></tr><tr><td>Peter Brass</td><td>1</td></tr><tr><td>Luciano Ramalho</td><td>1</td></tr><tr><td>Addy Osmani</td><td>1</td></tr><tr><td>Andriy Burkov</td><td>1</td></tr><tr><td>Tracy Kidder</td><td>1</td></tr><tr><td>Swain Scheps</td><td>1</td></tr><tr><td>François Chollet</td><td>1</td></tr><tr><td>Rod Stephens</td><td>1</td></tr><tr><td>Bernard Marr</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Mark W. Spong",
         1
        ],
        [
         "Chris Bernhardt",
         1
        ],
        [
         "Tariq Rashid",
         1
        ],
        [
         "Peter Brass",
         1
        ],
        [
         "Luciano Ramalho",
         1
        ],
        [
         "Addy Osmani",
         1
        ],
        [
         "Andriy Burkov",
         1
        ],
        [
         "Tracy Kidder",
         1
        ],
        [
         "Swain Scheps",
         1
        ],
        [
         "François Chollet",
         1
        ],
        [
         "Rod Stephens",
         1
        ],
        [
         "Bernard Marr",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "author",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_books",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT author, COUNT(book_id) total_books FROM books_streaming_temp_vw \n",
    "GROUP BY author;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2076ebbf-03a6-41ad-b7b2-29281d423291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-693258673127663>:7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m     display(df)\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n",
       "\n",
       "File \u001B[0;32m<command-693258673127663>:5\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n",
       "\u001B[1;32m      4\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mU0VMRUNUICogRlJPTSBib29rc19zdHJlYW1pbmdfdGVtcF92dyAKT1JERVIgQlkgYXV0aG9y\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n",
       "\u001B[0;32m----> 5\u001B[0m \u001B[43mdisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/display.py:76\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, DataFrame):\n",
       "\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124misStreaming\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n",
       "\u001B[0;32m---> 76\u001B[0m         \u001B[43mhandleStreamingDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqlContext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mentry_point\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m     78\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstreamName\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/display.py:122\u001B[0m, in \u001B[0;36mhandleStreamingDataFrame\u001B[0;34m(input, sc, sqlContext, entry_point, kwargs)\u001B[0m\n",
       "\u001B[1;32m    117\u001B[0m     trigger \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    119\u001B[0m name \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstreamName\u001B[39m\u001B[38;5;124m'\u001B[39m, jvm\u001B[38;5;241m.\u001B[39mcom\u001B[38;5;241m.\u001B[39mdatabricks\u001B[38;5;241m.\u001B[39mbackend\u001B[38;5;241m.\u001B[39mdaemon\u001B[38;5;241m.\u001B[39mdriver \\\n",
       "\u001B[1;32m    120\u001B[0m                   \u001B[38;5;241m.\u001B[39mDisplayHelper\u001B[38;5;241m.\u001B[39mgetStreamName())\n",
       "\u001B[0;32m--> 122\u001B[0m \u001B[43mentry_point\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetDriverSparkHooks\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisplayStreamingDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrigger\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    123\u001B[0m \u001B[43m                                                            \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcheckpointLocation\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode; line 2 pos 0;\n",
       "~Sort [author#7547 ASC NULLS FIRST], true\n",
       "+- ~Project [book_id#7545, title#7546, author#7547, category#7548, price#7549]\n",
       "   +- ~SubqueryAlias books_streaming_temp_vw\n",
       "      +- View (`books_streaming_temp_vw`, [book_id#7545,title#7546,author#7547,category#7548,price#7549])\n",
       "         +- ~SubqueryAlias spark_catalog.default.books\n",
       "            +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@285bc31c,CSV,List(),Some(StructType(StructField(book_id,StringType,true),StructField(title,StringType,true),StructField(author,StringType,true),StructField(category,StringType,true),StructField(price,DoubleType,true))),List(),None,Map(delimiter -> ;, header -> true, path -> dbfs:/mnt/demo-datasets/bookstore/books-csv/export_*.csv),Some(CatalogTable(\n",
       "Catalog: spark_catalog\n",
       "Database: default\n",
       "Table: books\n",
       "Owner: root\n",
       "Created Time: Thu Jan 09 08:45:18 UTC 2025\n",
       "Last Access: UNKNOWN\n",
       "Created By: Spark 3.3.2\n",
       "Type: EXTERNAL\n",
       "Provider: CSV\n",
       "Location: dbfs:/mnt/demo-datasets/bookstore/books-csv/export_*.csv\n",
       "Serde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n",
       "InputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\n",
       "OutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n",
       "Storage Properties: [delimiter=;, header=true]\n",
       "Schema: root\n",
       " |-- book_id: string (nullable = true)\n",
       " |-- title: string (nullable = true)\n",
       " |-- author: string (nullable = true)\n",
       " |-- category: string (nullable = true)\n",
       " |-- price: double (nullable = true)\n",
       "))), FileSource[dbfs:/mnt/demo-datasets/bookstore/books-csv/export_*.csv], [book_id#7545, title#7546, author#7547, category#7548, price#7549]\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-693258673127663>:7\u001B[0m\n\u001B[1;32m      5\u001B[0m     display(df)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n\nFile \u001B[0;32m<command-693258673127663>:5\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[1;32m      4\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mU0VMRUNUICogRlJPTSBib29rc19zdHJlYW1pbmdfdGVtcF92dyAKT1JERVIgQlkgYXV0aG9y\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n\u001B[0;32m----> 5\u001B[0m \u001B[43mdisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/display.py:76\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, DataFrame):\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124misStreaming\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n\u001B[0;32m---> 76\u001B[0m         \u001B[43mhandleStreamingDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqlContext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mentry_point\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstreamName\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/display.py:122\u001B[0m, in \u001B[0;36mhandleStreamingDataFrame\u001B[0;34m(input, sc, sqlContext, entry_point, kwargs)\u001B[0m\n\u001B[1;32m    117\u001B[0m     trigger \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    119\u001B[0m name \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstreamName\u001B[39m\u001B[38;5;124m'\u001B[39m, jvm\u001B[38;5;241m.\u001B[39mcom\u001B[38;5;241m.\u001B[39mdatabricks\u001B[38;5;241m.\u001B[39mbackend\u001B[38;5;241m.\u001B[39mdaemon\u001B[38;5;241m.\u001B[39mdriver \\\n\u001B[1;32m    120\u001B[0m                   \u001B[38;5;241m.\u001B[39mDisplayHelper\u001B[38;5;241m.\u001B[39mgetStreamName())\n\u001B[0;32m--> 122\u001B[0m \u001B[43mentry_point\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetDriverSparkHooks\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisplayStreamingDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrigger\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    123\u001B[0m \u001B[43m                                                            \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcheckpointLocation\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode; line 2 pos 0;\n~Sort [author#7547 ASC NULLS FIRST], true\n+- ~Project [book_id#7545, title#7546, author#7547, category#7548, price#7549]\n   +- ~SubqueryAlias books_streaming_temp_vw\n      +- View (`books_streaming_temp_vw`, [book_id#7545,title#7546,author#7547,category#7548,price#7549])\n         +- ~SubqueryAlias spark_catalog.default.books\n            +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@285bc31c,CSV,List(),Some(StructType(StructField(book_id,StringType,true),StructField(title,StringType,true),StructField(author,StringType,true),StructField(category,StringType,true),StructField(price,DoubleType,true))),List(),None,Map(delimiter -> ;, header -> true, path -> dbfs:/mnt/demo-datasets/bookstore/books-csv/export_*.csv),Some(CatalogTable(\nCatalog: spark_catalog\nDatabase: default\nTable: books\nOwner: root\nCreated Time: Thu Jan 09 08:45:18 UTC 2025\nLast Access: UNKNOWN\nCreated By: Spark 3.3.2\nType: EXTERNAL\nProvider: CSV\nLocation: dbfs:/mnt/demo-datasets/bookstore/books-csv/export_*.csv\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nStorage Properties: [delimiter=;, header=true]\nSchema: root\n |-- book_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- author: string (nullable = true)\n |-- category: string (nullable = true)\n |-- price: double (nullable = true)\n))), FileSource[dbfs:/mnt/demo-datasets/bookstore/books-csv/export_*.csv], [book_id#7545, title#7546, author#7547, category#7548, price#7549]\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode; line 2 pos 0;\n~Sort [author#7547 ASC NULLS FIRST], true\n+- ~Project [book_id#7545, title#7546, author#7547, category#7548, price#7549]\n   +- ~SubqueryAlias books_streaming_temp_vw\n      +- View (`books_streaming_temp_vw`, [book_id#7545,title#7546,author#7547,category#7548,price#7549])\n         +- ~SubqueryAlias spark_catalog.default.books\n            +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@285bc31c,CSV,List(),Some(StructType(StructField(book_id,StringType,true),StructField(title,StringType,true),StructField(author,StringType,true),StructField(category,StringType,true),StructField(price,DoubleType,true))),List(),None,Map(delimiter -> ;, header -> true, path -> dbfs:/mnt/demo-datasets/bookstore/books-csv/export_*.csv),Some(CatalogTable(\nCatalog: spark_catalog\nDatabase: default\nTable: books\nOwner: root\nCreated Time: Thu Jan 09 08:45:18 UTC 2025\nLast Access: UNKNOWN\nCreated By: Spark 3.3.2\nType: EXTERNAL\nProvider: CSV\nLocation: dbfs:/mnt/demo-datasets/bookstore/books-csv/export_*.csv\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nStorage Properties: [delimiter=;, header=true]\nSchema: root\n |-- book_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- author: string (nullable = true)\n |-- category: string (nullable = true)\n |-- price: double (nullable = true)\n))), FileSource[dbfs:/mnt/demo-datasets/bookstore/books-csv/export_*.csv], [book_id#7545, title#7546, author#7547, category#7548, price#7549]\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM books_streaming_temp_vw \n",
    "ORDER BY author; -- sorting is not supported by streaming temp view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53fe21ae-e636-4b98-b178-d99dd1103087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW author_count_temp_view -- author_count_temp_view is also streaming temporary view\n",
    "AS SELECT author,COUNT(book_id) total_books FROM books_streaming_temp_vw \n",
    "GROUP BY author;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42b8c5ec-55ad-4eef-a4eb-13fdae0507f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[120]: <pyspark.sql.streaming.query.StreamingQuery at 0x7fc1671dccd0>"
     ]
    }
   ],
   "source": [
    "(spark.table(\"author_count_temp_view\")                               \n",
    "      .writeStream  \n",
    "      .trigger(processingTime='4 seconds')\n",
    "      .outputMode(\"complete\") \n",
    "      .option(\"checkpointLocation\", \"dbfs:/mnt/demo-datasets/author_counts_checkpoint\")\n",
    "      .table(\"author_counts\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a566801a-bcd4-4dd2-ad34-6bd61e3d7fb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>author</th><th>total_books</th></tr></thead><tbody><tr><td>François Chollet</td><td>1</td></tr><tr><td>Chris Bernhardt</td><td>1</td></tr><tr><td>Luciano Ramalho</td><td>1</td></tr><tr><td>Mark W. Spong</td><td>1</td></tr><tr><td>Andriy Burkov</td><td>1</td></tr><tr><td>Tariq Rashid</td><td>1</td></tr><tr><td>Tracy Kidder</td><td>1</td></tr><tr><td>Swain Scheps</td><td>1</td></tr><tr><td>Rod Stephens</td><td>1</td></tr><tr><td>Bernard Marr</td><td>1</td></tr><tr><td>Peter Brass</td><td>1</td></tr><tr><td>Addy Osmani</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "François Chollet",
         1
        ],
        [
         "Chris Bernhardt",
         1
        ],
        [
         "Luciano Ramalho",
         1
        ],
        [
         "Mark W. Spong",
         1
        ],
        [
         "Andriy Burkov",
         1
        ],
        [
         "Tariq Rashid",
         1
        ],
        [
         "Tracy Kidder",
         1
        ],
        [
         "Swain Scheps",
         1
        ],
        [
         "Rod Stephens",
         1
        ],
        [
         "Bernard Marr",
         1
        ],
        [
         "Peter Brass",
         1
        ],
        [
         "Addy Osmani",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "author",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_books",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM author_counts; \n",
    "-- Note: This is not a streaming query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a2ea57c-7ad6-46c9-a1ca-8f74eaae80dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[131]: DataFrame[key: string, value: string]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SET spark.sql.hive.convertMetastoreParquet=false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "745886d3-329a-4d87-ab6c-493ae1ef0c1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-693258673127670>:7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m     display(df)\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n",
       "\n",
       "File \u001B[0;32m<command-693258673127670>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n",
       "\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mSU5TRVJUIElOVE8gYm9va3MgVkFMVUVTICgiQjE2IiwgIkhhbmRzLU9uIERlZXAgTGVhcm5pbmcgQWxnb3JpdGhtcyB3aXRoIFB5dGhvbiIsICJTdWRoYXJzYW4gUmF2aWNoYW5kaXJhbiIsICJDb21wdXRlciBTY2llbmNlIiwgMjUp\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m   display(df)\n",
       "\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Can only write data to relations with a single path."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-693258673127670>:7\u001B[0m\n\u001B[1;32m      5\u001B[0m     display(df)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n\nFile \u001B[0;32m<command-693258673127670>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mSU5TRVJUIElOVE8gYm9va3MgVkFMVUVTICgiQjE2IiwgIkhhbmRzLU9uIERlZXAgTGVhcm5pbmcgQWxnb3JpdGhtcyB3aXRoIFB5dGhvbiIsICJTdWRoYXJzYW4gUmF2aWNoYW5kaXJhbiIsICJDb21wdXRlciBTY2llbmNlIiwgMjUp\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m   display(df)\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Can only write data to relations with a single path.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Can only write data to relations with a single path.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "INSERT INTO books VALUES (\"B16\", \"Hands-On Deep Learning Algorithms with Python\", \"Sudharsan Ravichandiran\", \"Computer Science\", 25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aeff484-95a3-4259-9ed5-fc30361db07b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-693258673127667>:7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m     display(df)\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n",
       "\n",
       "File \u001B[0;32m<command-693258673127667>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n",
       "\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mLS0gVGhlIGdyYXBoIG9mIHN0cmVhbWluZyBxdWVyeSBrZWVwcyBjaGFuZ2luZyBhcyB0aGUgbmV3IGRhdGEgYXJyaXZlcyAKLS0gbGV0cyBpbnNlcnQgbmV3IGRhdGEgaW4gb3VyIHNvdXJjZSB0YWJsZShib29rcykKCklOU0VSVCBJTlRPIGJvb2tzIFZBTFVFUyAoCigiQjE5IiwgIkludHJvZHVjdGlvbiB0byBNb2RlbGluZyBhbmQgU2ltdWxhdGlvbiIsICJNYXJrIFcuIFNwb25nIiwgIkNvbXB1dGVyIFNjaWVuY2UiLCAyNSksCigiQjIwIiwgIlJvYm90IE1vZGVsaW5nIGFuZCBDb250cm9sIiwgIk1hcmsgVy4gU3BvbmciLCAiQ29tcHV0ZXIgU2NpZW5jZSIsIDMwKSwKKCJCMjEiLCAiVHVyaW5nJ3MgVmlzaW9uOiBUaGUgQmlydGggb2YgQ29tcHV0ZXIgU2NpZW5jZSIsICJDaHJpcyBCZXJuaGFyZHQiLCAiQ29tcHV0ZXIgU2NpZW5jZSIsIDM1KQop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m   display(df)\n",
       "\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Can only write data to relations with a single path."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-693258673127667>:7\u001B[0m\n\u001B[1;32m      5\u001B[0m     display(df)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n\nFile \u001B[0;32m<command-693258673127667>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mLS0gVGhlIGdyYXBoIG9mIHN0cmVhbWluZyBxdWVyeSBrZWVwcyBjaGFuZ2luZyBhcyB0aGUgbmV3IGRhdGEgYXJyaXZlcyAKLS0gbGV0cyBpbnNlcnQgbmV3IGRhdGEgaW4gb3VyIHNvdXJjZSB0YWJsZShib29rcykKCklOU0VSVCBJTlRPIGJvb2tzIFZBTFVFUyAoCigiQjE5IiwgIkludHJvZHVjdGlvbiB0byBNb2RlbGluZyBhbmQgU2ltdWxhdGlvbiIsICJNYXJrIFcuIFNwb25nIiwgIkNvbXB1dGVyIFNjaWVuY2UiLCAyNSksCigiQjIwIiwgIlJvYm90IE1vZGVsaW5nIGFuZCBDb250cm9sIiwgIk1hcmsgVy4gU3BvbmciLCAiQ29tcHV0ZXIgU2NpZW5jZSIsIDMwKSwKKCJCMjEiLCAiVHVyaW5nJ3MgVmlzaW9uOiBUaGUgQmlydGggb2YgQ29tcHV0ZXIgU2NpZW5jZSIsICJDaHJpcyBCZXJuaGFyZHQiLCAiQ29tcHV0ZXIgU2NpZW5jZSIsIDM1KQop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m   display(df)\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Can only write data to relations with a single path.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Can only write data to relations with a single path.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- The graph of streaming query keeps changing as the new data arrives \n",
    "-- lets insert new data in our source table(books)\n",
    "\n",
    "INSERT INTO books\n",
    " values (\"B16\", \"Hands-On Deep Learning Algorithms with Python\", \"Sudharsan Ravichandiran\", \"Computer Science\", 25),\n",
    "        (\"B17\", \"Neural Network Methods in Natural Language Processing\", \"Yoav Goldberg\", \"Computer Science\", 30),\n",
    "        (\"B18\", \"Understanding digital signal processing\", \"Richard Lyons\", \"Computer Science\", 35);"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 693258673127671,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "STRUCTURED STREAMING HANDS ON - Udemy",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
